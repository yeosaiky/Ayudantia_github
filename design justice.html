<!DOCTYPE html>
<head>
<link rel="stylesheet" href="design justice.css"> 
<meta charset="UTF-8">
<link href="https://fonts.googleapis.com/css2?family=Anton&display=swap" rel="stylesheet">
</head>

<header>
<h1>Design justice</h1>
</header>
<link rel="stylesheet" href="estilos.css">
<main>
    
<body>
<h2>Sasha Contanza-Chock</h2>
<section class="bloque"> 
<p>Sasha Costanza-Chock (elle o ella) es une investigadore y diseñadore cuyo trabajo se basa en apoyar procesos comunitarios para construir y redistribuir el poder, avanzando hacia la liberación colectiva y una ecología sostenible. El trabajo de Sasha se centra en movimientos sociales en las redes, medios de comunicación como herramientas para comunidades en lucha, y justicia en procesos de diseño. 
</p>
<img src="sasha.jpeg" class="img-bloque" alt="imagen">
<p> 
Su primer libro “Out of the Shadows, Into the Streets Transmedia Organizing and the Immigrant Rights Movement" (2014), trata de la tecnología y el movimiento de los derechos de los migrantes. Su segundo libro "Design Justice: Community-led Practices To Build The Worlds We Need" (2020) se enfoca en la justicia del diseño, utilizando prácticas comunitarias para crear los mundos que necesitamos. Ambos libros abordan temas relevantes para la organización social y la justicia. Por otro lado en su visita a Chile por el Mes del Diseño, Sasha realizó una charla abierta en la Escuela de Diseño UC donde habló sobre el enfoque del Diseño Justo, «un marco de análisis para todos los procesos de diseño”, que busca rediseñar las prácticas del diseño, centrándose en poder dar voz, representar y satisfacer los deseos y necesidades de las comunidades usualmente marginadas por la estructura social del sistema capitalista.
</p>

<h4>"Design Justice, A.I., and Escape from the Matrix of Domination"</h4>

<p>En el artículo "Design Justice, A.I., and Escape from the Matrix of Domination", Costanza-Chock nos habla de cómo las tecnologías de inteligencia artificial amplifican estructuras de opresión como el racismo, el sexismo, el capacitismo y la transfobia. Nos expone la Justicia de Diseño de manera ética y política que reestructura el diseño tecnológico desde las comunidades oprimidas. El texto hace un llamado a la “matriz de dominación” que guía el desarrollo tecnológico, y a construir prácticas de diseño más inclusivas, equitativas y colectivas, Además la justicia del diseño replantea los procesos de diseño, centra a las personas que normalmente son marginadas por el diseño y utiliza prácticas colaborativas y creativas para abordar los desafíos más profundos que enfrentan nuestras comunidades.
</p>
</section>

     
<h2>“Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification”. Joy Buolamwini, Timnit Gebru </h2> 
<section class="bloque2">
<img src="proyecto2.jpg" class="img-bloque2" alt="imagen">
<p>Joy Buolamwini es una destacada informática y activista digital de origen ghanesa-estadounidense, reconocida por su lucha en contra de los sesgos dentro del algoritmo en la inteligencia artificial. Siendo cofundadora de Algorithmic Justice League (AJL) ayuda a garantizar que el que las nuevas tecnologías IA no cometan el error de no ser neutrales y sigan con prejuicios con prejuicios ante minorías. Su investigación sobre el sesgo en los sistemas de reconocimiento facial donde se descubrió que las tecnologías fallaban por estar mal programadas al no incluir a personas negras dentro de los datos para poder reconocerlas bien, esto la llevó a fundar la campaña “Stop the Bias”, que busca concientizar sobre los riesgos del sesgo algorítmico y promover prácticas más justas e inclusivas en el desarrollo de la IA.
</p>

<p>Timnit Gebru es una científica informática de origen etíope-estadounidense, especializada en el sesgo en los algoritmos, trabajando para que las personas negras sean representadas en la recolección de datos de la IA, logrando ser cofundadora de “Black in AI” que tiene el mismo fin. Por un tiempo trabajó en el equipo de IA ética de google, pero fue despedida por un conflicto sobre otro artículo del que era coautora.
</p>

<h4>explicación del proyecto</h4>

<p>Este proyecto tuvo como objetivo evaluar el sesgo presente en los algoritmos y conjuntos de datos de análisis facial automatizado con respecto a subgrupos fenotípicos. Sus autores utilizaron el sistema de clasificación de tipo de piel de Fitzpatrick, aprobado por dermatólogos para caracterizar la distribución de género y tipo de piel en dos conjuntos de datos de referencia existentes, IJB-A y Adience, descubrieron que estos conjuntos de datos estaban abrumadoramente compuestos por sujetos de piel más clara
</p>

<p>El hallazgo clave del estudio es que las mujeres de piel más oscura son el grupo más mal clasificado por estos sistemas, con tasas de error de hasta el 34,7%. En contraste la tasa de error máxima para los hombres de piel más clara fue solo del 0,8%, en general todos los clasificadores funcionaron mejor en rostros masculinos que femeninos, y mejor en rostros más claros que más oscuros. 
</p>

<h4>Análisis</h4>

<p>Tanto el paper como el proyecto hacen un llamado a que las inteligencias artificiales y diseño tecnológico perpetúan los prejuicios sociales, el cómo las personas negras como personas trans suelen ser perjudicadas bajo estos sistemas. Los trabajos indican que es momento de actuar con sistemas más inclusivos, a no dejar de lado a minorías en sus sistemas de datos y no seguir con sesgos.
</p>

<p>Ahora, por un lado el trabajo de Costanza-Chock "Design Justice, A.I., and Escape from the Matrix of Domination" tiene un enfoque en cómo el diseño de las nuevas tecnologías pueden ser discriminatorias, si no se piensa en los grupos más marginados. Por el otro , el trabajo de Joy Buolamwini y Timnit Gebru “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification” habla específicamente sobre la IA dentro del sistema de clasificación de género.  
</p>
</section>  
  
<h2>“Machine learning bias”</h2>
<div class="contenedor-dos-columnas">
<div class="columna texto">
<section class="bloque">

<p>Es un proyecto de Nushin Isabelle Yazdani el cual explora el uso de modelos de aprendizaje automatizado para generar representaciones visuales que evidencian cómo la discriminación estructural presente en el sistema de justicia penal es perpetuada a través de algoritmos predictivos. Utiliza el modelo DCGAN (Deep Convolutional Generative Adversarial Network), el cual recopiló aproximadamente 3.000 fotografías de reclusos estadounidenses, con el fin de producir imágenes futuras de próximos prisioneros. Utilizando el enfoque de hacer tangible la controversia al tema de predicción delictiva, lo cual refuerza sesgos raciales y sociales preexistentes, junto con una gran discriminación.Es un proyecto de Nushin Isabelle Yazdani el cual explora el uso de modelos de aprendizaje automatizado para generar representaciones visuales que evidencian cómo la discriminación estructural presente en el sistema de justicia penal es perpetuada a través de algoritmos predictivos. Utiliza el modelo DCGAN (Deep Convolutional Generative Adversarial Network), el cual recopiló aproximadamente 3.000 fotografías de reclusos estadounidenses, con el fin de producir imágenes futuras de próximos prisioneros. Utilizando el enfoque de hacer tangible la controversia al tema de predicción delictiva, lo cual refuerza sesgos raciales y sociales preexistentes, junto con una gran discriminación.
</p>

<h4>Es importante aquí distinguir entre tres tipos de sesgo algorítmico:</h4>
<ul>
<li>Los que dependen del programador</li>
<li>Los que dependen de la construcción de las bases de datos</li>
<li>Los que dependen de factores históricos y sociales</li>

<h4>Nushin Isabelle Yazdan</h4>

<p>Nushin Isabelle Yazdani (ella) es una diseñadora de transformación, artista e investigadora de diseño de IA. Su trabajo está centralizado en la intersección del aprendizaje automático (machine learning), el diseño para la justicia social y las prácticas feministas interseccionales, a través de este lente busca crear y explorar procesos de diseño que desmantelen estructuras opresivas en sistemas de IA. En este proceso enfatiza la colaboración con las comunidades que son directamente afectadas por los resultados de diseño de la IA.
</p>

<h4>Análisis</h4>
<p>Ambas autoras alzan la voz por los grupos discriminados, haciendo visible la estructura con la que la IA trabaja, siendo esta una manera totalmente discriminatoria. El concepto de la “matriz de dominación” planteado por Costanza-Chock se hace visible en el trabajo de Nushin Yazdani, donde el diseño algorítmico basado en datos racistas produce predicciones sesgadas que pueden impactar la vida de personas negras y racializadas. Las autoras critican profundamente el papel que juega la AI, y buscan la reducción de estas estructuras opresivas, coinciden en que no basta con “ajustar” la tecnología: es necesario rediseñar desde la raíz, con procesos que incluyan a las comunidades discriminadas por las tecnologías. 
</p>

<p>Una diferencia que existe entre ambos trabajos es el enfoque de diferentes tecnologías actuales y a que grupos sociales afecta. Por un lado, Constanza-Chock trata sobre cómo el diseño de tecnologías no están adaptados y no son gentiles con las minorías, fomentando desigualdades sociales y siguiendo con estereotipos. Por el otro, Nushin Yazdani habla de como la base de datos de la inteligencia artificial refuerzan los sesgos raciales y de clases sociales. 
</p>
</div>

<div class="columna imagen">
<section class="bloque">
<img src="proyecto1.jpg">
<img src="justice2.png">
<p>
  
</p>
</div>
</div>

<div class="bloque-inferior">
<h2>Conclusiones</h2>
<p>Ambos trabajos crean conciencia de cómo las nuevas tecnologías aún les falta camino para ser inclusivas, el hecho de que en nuestra cultura se sigan perpetuando estas injusticias y discriminaciones que hace años se pensaron ya estar extintas. Es responsabilidad de los desarrolladores incluir y no estar sesgados por estereotipos, dirigirse a un mundo más equitativo y justo. 
</p>
<p>Con respecto al diseño se deben desarrollar las metodologías para que estos fallos no continúen y perjudiquen, también ayudará si incluimos a minorías a participar en decisiones de diseño si es que los desarrolladores necesitan una opinión desde la experiencia. Si empezamos a tomar en serio estos trabajos las nuevas tecnologías dejarán de excluir a gente y dejaran de seguir los prejuicios, este sería el verdadero desarrollo dentro de la IA y las tecnologías en la sociedad. 
</p>

<p>Por todo lo abordado anteriormente que tener en cuenta la justicia del diseño en los procesos es una decisión ética y una necesidad para que todas aquellas herramientas tecnológicas no sigan reproduciendo discriminación y violencia hacia comunidades oprimidas, el proyecto de Yazdani evidencia como la IA tiene mucho que cambiar dentro de su algoritmo y el texto de Costanza-Chock muestra posibles maneras para rediseñar sistemas desde lo interseccional, lo colectivo y comunitario. Desde este punto de vista el diseño no es solo un herramienta estética y funcional, si no que es una herramienta crítica capaz de transformar y contribuir de manera activa a una cultura y sociedad más justa, consciente y equitativa.
</p>
</div>    
</div>

</section>
</main>

</body>
</html>

       

    